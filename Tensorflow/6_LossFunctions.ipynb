{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "6_LossFunctions.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMD8+XjwpF8Z6zxKjL7rpv/",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jay-thakur/DataScienceTutorial/blob/main/Tensorflow/6_LossFunctions.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ziwwH0kpOgjh",
        "outputId": "14cd6bcf-4ec6-49bc-8e98-afe1a656c9b3"
      },
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "print(\"TensorFlow version:\", tf.__version__)"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TensorFlow version: 2.7.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EP7EwXjhiHRg"
      },
      "source": [
        "# Loss Functions\n",
        "Given an input vector, a loss function is a measure of how bad a particular model performs in predicting a desired output  quantity (regression) or correctly labeling the input vector (classification).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A4gZaGYuHCAk"
      },
      "source": [
        "## Mean Squared Error (MSE).\n",
        "\n",
        "Mean Squared Error is the most commonly used regression loss function and it is defined as the mean of the squared distances between the desired and the predicted output(s).\n",
        "\n",
        "$$\\large MSE=\\frac{1}{N}\\sum_{i=1}^{N} {(t_i-y_i)}^2$$\n",
        "\n",
        "where $N$ is the total number of samples , $t_i$ is the desired output for sample $i$ and $y_i$ is the actual output for sample $i$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EL5Lcy_8Dfr_",
        "outputId": "205cf8fc-6923-4c6b-d53f-4686462cce6c"
      },
      "source": [
        "y_true = tf.Variable(np.random.randint(0, 2, size=(2, 3)))\n",
        "y_pred = tf.Variable(np.random.random(size=(2, 3)))\n",
        "\n",
        "y_true = tf.cast(y_true, y_pred.dtype)\n",
        "\n",
        "def mean_squared_error(y_true, y_pred):\n",
        "  return tf.reduce_mean(tf.math.square(y_pred - y_true))\n",
        "\n",
        "print(\"Mean Squared Error :: \", mean_squared_error(y_true, y_pred).numpy())\n",
        "\n",
        "# Mean Squared Error using Keras API\n",
        "mse = tf.keras.losses.MeanSquaredError()\n",
        "print(\"Mean Squared Error using Keras API ::\", mse(y_true, y_pred).numpy())"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mean Squared Error ::  0.2618975787629521\n",
            "Mean Squared Error using Keras API :: 0.26189759373664856\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Emh7J3vgHHUi"
      },
      "source": [
        "## Mean Absolute Error (MAE).\n",
        "\n",
        "Mean Absolute Error is another common function used for regression models and it is defined as as the mean of the absolute differences between the desired and the predicted output(s).\n",
        "\n",
        "$$\\large MAE=\\frac{1}{N}\\sum_{i=1}^{N} {|t_i-y_i|}$$\n",
        "\n",
        "where $N$ is the total number of samples , $t_i$ is the desired output for sample $i$ and $y_i$ is the actual output for sample $i$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DYVa_BorLhcX",
        "outputId": "d5b16019-4646-4788-fac8-f7dbdfa88a90"
      },
      "source": [
        "y_true = tf.Variable(np.random.randint(0, 2, size=(2, 3)))\n",
        "y_pred = tf.Variable(np.random.random(size=(2, 3)))\n",
        "\n",
        "y_true = tf.cast(y_true, y_pred.dtype)\n",
        "\n",
        "def mean_absolute_error(y_true, y_pred):\n",
        "  return tf.reduce_mean(tf.math.abs(y_pred - y_true))\n",
        "\n",
        "print(\"Mean Absolute Error :: \", mean_absolute_error(y_true, y_pred).numpy())\n",
        "\n",
        "# Mean Absolute Error using Keras API\n",
        "mae = tf.keras.losses.MeanAbsoluteError()\n",
        "print(\"Mean Absolute Error using Keras API ::\", mae(y_true, y_pred).numpy())"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mean Absolute Error ::  0.5366752401709879\n",
            "Mean Absolute Error using Keras API :: 0.536675214767456\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WaVG6qYCiL_m"
      },
      "source": [
        "## Hinge (Multiclass Support Vector Machine) Loss\n",
        "\n",
        "The hinge loss function is used for classification and it is based on the concept of maximum-margin. The hinge loss for sample number $\\large s$ is formulated as:\n",
        "\n",
        "\n",
        "$$\\large L_s=\\sum_{j \\neq s_t}^{C} max(0,y_j-y_{s_t}+\\Delta)$$\n",
        "\n",
        "where $\\large s$ is the sample number, $\\large C$ is the number of classes, and  $\\large s_t$ is the index of the true class for sample number $\\large s$, and $\\Delta$ is a constant.\n",
        "\n",
        "\n",
        "The total loss across all the samples can be calculated as:\n",
        "\n",
        "$$\\large L=\\frac {1}{N} \\sum_{s} L_s$$\n",
        "\n",
        "\n",
        "where  $N$ is the number of the samples, $L$ is the total loss over all the samples and $s$ is the sample number\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FYzBD_rFthiP",
        "outputId": "b7c8d4e5-27a9-4599-b582-c16eeeb195b0"
      },
      "source": [
        "y_true = tf.Variable([[0., 1.], [0., 0.]])\n",
        "y_pred = tf.Variable([[0.6, 0.4], [0.4, 0.6]])\n",
        "\n",
        "hinge_loss = tf.keras.losses.Hinge()\n",
        "\n",
        "print(\"Hinge Loss :: \", hinge_loss(y_true, y_pred).numpy())"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hinge Loss ::  1.3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CMZtNtHjicXo"
      },
      "source": [
        "## Cross Entropy Loss\n",
        "\n",
        "The cross entropy loss is used for classification and it uses a softmax function to calculate the loss. \n",
        "\n",
        "### Softmax Function\n",
        "\n",
        "The softmax function gives a probabilistic interpretation to the output values and it is formulated as:\n",
        "\n",
        "$$\\large S(i)=\\frac{e^{y_i}}{\\sum_{j}^{C}e^{y_j}}$$\n",
        "\n",
        "where $\\large S(i)$ is the softmax value corresponding to the output $\\large y_i$, and $ C$ is the number of classes. This function interprets the outputs as unnormalized log probabilities of each class. Notice that the denominator of the above equation normalizes the probabilities so the total sums to 1. \n",
        "\n",
        "In other words the softmax function takes a vector of floating point numbers and proportionally compresses each number between zero and one such that the total adds up to 1.\n",
        "\n",
        "Using the softmax function, the cross entropy loss for sample $\\large s$ can be calculated as:\n",
        "\n",
        "$$\\large L_s=-log(\\frac{e^{y_{s_t}}}{\\sum_{j=1}^{C} e^{y_j}})$$\n",
        "\n",
        "\n",
        "where $\\large s$ is the sample number, $ C$ is the number of classes, and  $\\large s_t$ is the index of the true class for sample number $\\large s$.\n",
        "\n",
        "\n",
        "The above equation is really a simplified version of a discrete cross entropy between two distributions.\n",
        "\n",
        "Let's imagine that we have a true discrete distribution $p$ and an estimated discrete distribution $q$. The cross entropy between these two distribution is defined as:\n",
        "\n",
        "$$\\large H(p,q)= - \\sum_{x}p(x)log(q(x))$$\n",
        "\n",
        "Notice that in a multi-class classification problem the true probability distribution has all zeros except for the correct class, $i$, which has the value of 1:\n",
        "\n",
        "$$\\large p=[0,0,..., 1, ... 0]$$\n",
        "\n",
        "If the above discrete distribution $p$ is substituted into the general cross entropy equation it will result in the simplified cross entropy loss \n",
        "\n",
        "$$\\large L_s=-log(\\frac{e^{y_{s_t}}}{\\sum_{j=1}^{C} e^{y_j}})$$\n",
        "\n",
        "where $\\large s_t$ is the index of the correct class.\n",
        "\n",
        "Notice that to calculate the overall loss we still need to average the loss over all the samples.\n",
        "$$\\large L=\\frac {1}{N} \\sum_{s} L_s$$\n",
        "\n",
        "\n",
        "where $ N$  is the number of samples and $L_s$ is the cross entropy loss for sample $s$\n",
        "\n",
        "**Note:** There is no \"softmax loss\". The correct terminology is \"cross-entropy loss\". The \"cross entropy loss\" uses the \"softmax\" function to calculate the loss.  \n",
        "\n",
        "<br>\n",
        "Cross Entropy can be applied in both Binary & Multicalss tasks."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zVX0MM7MMLBd",
        "outputId": "94f1cc7f-b752-43ad-b318-b8b908c3a320"
      },
      "source": [
        "y_true = tf.Variable([0, 1, 0, 0])\n",
        "y_pred = tf.Variable([-18.6, 0.51, 2.94, -12.8])\n",
        "\n",
        "bce = tf.keras.losses.BinaryCrossentropy(from_logits=True)\n",
        "\n",
        "print(\"Binary Cross Entropy Loss : \", bce(y_true, y_pred).numpy())"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Binary Cross Entropy Loss :  0.865458\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gr3gjHqeN-Hd"
      },
      "source": [
        "The `from_logits=True` attribute inform the loss function that the output values generated by the model are not normalized, a.k.a. logits. In other words, the softmax function has not been applied on them to produce a probability distribution. [learn more](https://datascience.stackexchange.com/questions/73093/what-does-from-logits-true-do-in-sparsecategoricalcrossentropy-loss-function)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QWYyeJooONr2"
      },
      "source": [
        "There are many cross entropy loss functions in tensorflow. so **How to choose cross entropy loss in tensorflow?**.\n",
        "\n",
        "[please follow this link to understand it.](https://stackoverflow.com/questions/47034888/how-to-choose-cross-entropy-loss-in-tensorflow)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e8ogNkON7qMG"
      },
      "source": [
        "# References\n",
        "\n",
        "\n",
        "https://www.tensorflow.org/api_docs/python/tf/math/reduce_mean\n",
        "\n",
        "https://www.tensorflow.org/api_docs/python/tf/losses\n",
        "\n",
        "https://www.tensorflow.org/api_docs/python/tf/nn\n",
        "\n",
        "https://datascience.stackexchange.com/questions/73093/what-does-from-logits-true-do-in-sparsecategoricalcrossentropy-loss-function\n",
        "\n",
        "https://stackoverflow.com/questions/47034888/how-to-choose-cross-entropy-loss-in-tensorflow\n",
        "\n",
        "\n",
        "https://github.com/farhadkamangar/CSE5368 \n",
        "\n",
        "https://cognitiveclass.ai/courses/course-v1:BigDataUniversity+ML0120EN+v2\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I6I09LS87sXU"
      },
      "source": [
        ""
      ],
      "execution_count": 5,
      "outputs": []
    }
  ]
}