{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "5_GradientTape.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "54VIFhYmOG4c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5dde2bf2-884f-4e58-fd02-d48b7455d08d"
      },
      "source": [
        "import tensorflow as tf\n",
        "print(\"TensorFlow version:\", tf.__version__)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TensorFlow version: 2.6.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3NB0KsPkwP8G"
      },
      "source": [
        "One of the advantage of Tensorflow is that It has Automatic Differentiation and gradients feature.\n",
        "\n",
        "so Let's understand first **What do we mean by Automatic Differentiation ?**."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3lBF8eF5l4WA"
      },
      "source": [
        "# What is Automatic differentiation and gradients ?\n",
        "\n",
        "<a href=\"https://en.wikipedia.org/wiki/Automatic_differentiation\">Automatic Differentiation</a> is a set of techniques to evaluate the derivative of a function specified by computer program."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S3o5TSVCm4MG"
      },
      "source": [
        "# How Automatic differentiation is useful in Neural Network ?\n",
        "\n",
        "Automatic differentiation is useful for implementing machine learning algorithms such as backpropagation for training neural networks"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nK9tZhrUDkZ5"
      },
      "source": [
        "# How to compute gradients with Tensorflow?\n",
        "\n",
        "To differentiate automatically, TensorFlow needs to remember what operations happen in what order during the forward pass. Then, during the backward pass, TensorFlow traverses this list of operations in reverse order to compute gradients.\n",
        "\n",
        "\n",
        "TensorFlow provides the `tf.GradientTape` API for automatic differentiation; that is, computing the gradient of a computation with respect to some inputs, usually `tf.Variables`. TensorFlow \"records\" relevant operations executed inside the context of a `tf.GradientTape` onto a \"tape\". TensorFlow then uses that tape to compute the gradients of a \"recorded\" computation using reverse mode differentiation."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zQMEe2WFogX5"
      },
      "source": [
        "Let's see an example - "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GPPk5XoGoq4B"
      },
      "source": [
        "x = tf.Variable(3.0)\n",
        "\n",
        "with tf.GradientTape() as tape:\n",
        "  y = x**2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RWsQa1DLotgW"
      },
      "source": [
        "Once we've recorded some operations, we can use `GradientTape.gradient(target, sources)` to calculate the gradient of some target (often a loss) relative to some source (often the model's variables)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_cmYjtsoo9Bm",
        "outputId": "e1618de5-f5a3-4144-d8f9-05690db5d499"
      },
      "source": [
        "# dy = 2x * dx\n",
        "dy_dx = tape.gradient(y, x)\n",
        "dy_dx.numpy()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "6.0"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zdPfjq8fEHRc",
        "outputId": "21962a56-4154-427c-cefc-456b43a55aea"
      },
      "source": [
        "# combined code\n",
        "x = tf.Variable(3.0)\n",
        "\n",
        "with tf.GradientTape() as tape:\n",
        "  y = x**2\n",
        "\n",
        "# dy = 2x * dx\n",
        "dy_dx = tape.gradient(y, x)\n",
        "print(dy_dx.numpy())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "6.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m7wFBMzMqgWC"
      },
      "source": [
        "### Working of GradientTape on Tensor"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E9aIzeacqs_w",
        "outputId": "6e08cbb2-ff25-468b-aa62-7a16f4e29b2d"
      },
      "source": [
        "w = tf.Variable(tf.random.normal((3, 2)), name='w')\n",
        "b = tf.Variable(tf.zeros(2, dtype=tf.float32), name='b')\n",
        "x = [[1., 2., 3.]]\n",
        "\n",
        "with tf.GradientTape(persistent=True) as tape:\n",
        "  y = x @ w + b\n",
        "  loss = tf.reduce_mean(y**2)\n",
        "\n",
        "[dl_dw, dl_db] = tape.gradient(loss, [w, b])\n",
        "print(w.shape)\n",
        "print(dl_dw.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(3, 2)\n",
            "(3, 2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jFcKIOfvqy_S"
      },
      "source": [
        "Here to get the gradient of `loss` with respect to both variables, we can pass both as sources to the `gradient` method."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2s91MqkmrJW2"
      },
      "source": [
        "We can also pass a dictionary of variables like this - "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vSM0ZFXTrNx5",
        "outputId": "2c6c4a50-9c56-4c50-8790-9d8836e77ceb"
      },
      "source": [
        "my_vars = {\n",
        "    'w': w,\n",
        "    'b': b\n",
        "}\n",
        "\n",
        "grad = tape.gradient(loss, my_vars)\n",
        "grad['b']"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(2,), dtype=float32, numpy=array([6.556717 , 1.8888472], dtype=float32)>"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z45kt6p8sXxp"
      },
      "source": [
        "# Default tape behaviour & its reason.\n",
        "\n",
        "The default behavior is to record all operations after accessing a trainable tf.Variable. The reasons for this are:\n",
        "\n",
        "* The tape needs to know which operations to record in the forward pass to calculate the gradients in the backwards pass.\n",
        "*  The tape holds references to intermediate outputs, so you don't want to record unnecessary operations.\n",
        "*  The most common use case involves calculating the gradient of a loss with respect to all a model's trainable variables.\n",
        "\n",
        "\n",
        "Let's see the below example which fails to calculate a gradient because the tf.Tensor is not \"watched\" by default, and the `tf.Variable` is not trainable. (Notice `trainable=False`)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UMbUnJiRtKYi",
        "outputId": "6fec2a61-a8ed-48bc-cc68-9c9bad1caa26"
      },
      "source": [
        "# A trainable variable\n",
        "x0 = tf.Variable(3.0, name='x0')\n",
        "\n",
        "# Not trainable\n",
        "x1 = tf.Variable(3.0, name='x1', trainable=False)\n",
        "\n",
        "# Not a Variable: A variable + tensor returns a tensor.\n",
        "x2 = tf.Variable(2.0, name='x2') + 1.0\n",
        "\n",
        "# Not a variable\n",
        "x3 = tf.constant(3.0, name='x3')\n",
        "\n",
        "with tf.GradientTape() as tape:\n",
        "  y = (x0**2) + (x1**2) + (x2**2)\n",
        "\n",
        "grad = tape.gradient(y, [x0, x1, x2, x3])\n",
        "\n",
        "for g in grad:\n",
        "  print(g)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tf.Tensor(6.0, shape=(), dtype=float32)\n",
            "None\n",
            "None\n",
            "None\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ukS1JFhktgxJ"
      },
      "source": [
        "# How to know what all variables are being watched by the tape?\n",
        "\n",
        "we can list the variables being watched by the tape using the `GradientTape.watched_variables` method."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dX7TW-yTts2M",
        "outputId": "778bd40b-57c8-427e-ad77-bbf5cde2273f"
      },
      "source": [
        "[var.name for var in tape.watched_variables()]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['x0:0']"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4tXISqCCsLXu"
      },
      "source": [
        "# How to control, What the tape watches ?\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5RYiUFU_uAad"
      },
      "source": [
        "To record gradients with respect to a `tf.Tensor`, we need to call `GradientTape.watch(x)`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HbruVS9OuFlU",
        "outputId": "69b3ecd8-2273-47ad-8790-c68af87e3c72"
      },
      "source": [
        "x = tf.constant(3.0)\n",
        "with tf.GradientTape() as tape:\n",
        "  tape.watch(x)\n",
        "  y = x**2\n",
        "\n",
        "# dy = 2x * dx\n",
        "dy_dx = tape.gradient(y, x)\n",
        "print(dy_dx.numpy())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "6.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dc_k_ja8uNDX"
      },
      "source": [
        "Conversely, to disable the default behavior of watching all `tf.Variables`, set `watch_accessed_variables=False` when creating the gradient tape. This calculation uses two variables, but only connects the gradient for one of the variables."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f_QPhqVRuSMV",
        "outputId": "da1087fb-f240-4e0a-829b-29e4577e46fe"
      },
      "source": [
        "x0 = tf.Variable(0.0)\n",
        "x1 = tf.Variable(10.0)\n",
        "\n",
        "with tf.GradientTape(watch_accessed_variables=False) as tape:\n",
        "  tape.watch(x1)\n",
        "  y0 = tf.math.sin(x0)\n",
        "  y1 = tf.nn.softplus(x1)\n",
        "  y = y0 + y1\n",
        "  ys = tf.reduce_sum(y)\n",
        "\n",
        "# dys/dx1 = exp(x1) / (1 + exp(x1)) = sigmoid(x1)\n",
        "grad = tape.gradient(ys, {'x0': x0, 'x1': x1})\n",
        "\n",
        "print('dy/dx0:', grad['x0'])\n",
        "print('dy/dx1:', grad['x1'].numpy())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dy/dx0: None\n",
            "dy/dx1: 0.9999546\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tjOKo85yuZxD"
      },
      "source": [
        "**Notice** Since `GradientTape.watch` was not called on x0, no gradient is computed with respect to it."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LwYdVS01FE3s"
      },
      "source": [
        "# How to compute Higher Order Derivatives ?\n",
        "\n",
        "we can compute higher-order derivatives by nesting tapes."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d5oGUtrtiTlh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9ce57f10-bb0a-452f-c1f2-61d4f581b478"
      },
      "source": [
        "a = tf.Variable(tf.random.normal(shape=(2, 2)))\n",
        "b = tf.Variable(tf.random.normal(shape=(2, 2)))\n",
        "\n",
        "with tf.GradientTape() as outer_tape:\n",
        "  with tf.GradientTape() as tape:\n",
        "    c = tf.sqrt(tf.square(a) + tf.square(b))\n",
        "    dc_da = tape.gradient(c, a)\n",
        "  d2c_da2 = outer_tape.gradient(dc_da, a)\n",
        "  print(d2c_da2)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tf.Tensor(\n",
            "[[0.31170034 0.32279664]\n",
            " [1.153167   0.56869346]], shape=(2, 2), dtype=float32)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gc-0VJ-GCuIq"
      },
      "source": [
        "# References\n",
        "\n",
        "https://www.tensorflow.org/api_docs/python/tf/GradientTape\n",
        "\n",
        "https://www.tensorflow.org/guide/autodiff\n",
        "\n",
        "\n",
        "https://github.com/farhadkamangar/CSE5368 \n",
        "\n",
        "https://cognitiveclass.ai/courses/course-v1:BigDataUniversity+ML0120EN+v2\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qUfMR0KSY8r3"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}